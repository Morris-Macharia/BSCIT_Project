import pandas as pd
import numpy as np
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.combine import SMOTETomek
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve

# Load dataset
df = pd.read_csv("insurance_claims.csv")
print(f"Total records loaded: {len(df)}")

# Drop irrelevant columns
df.drop(columns=['policy_number', 'insured_zip', 'auto_make', 'auto_model', 'auto_year'], inplace=True)

# Handle missing values
df.replace('?', np.nan, inplace=True)
for col in df.select_dtypes(include='object').columns:
    df[col] = df[col].fillna(df[col].mode()[0])
for col in df.select_dtypes(include=[np.number]).columns:
    df[col] = df[col].fillna(df[col].median())

# Convert date columns to features
df['policy_bind_date'] = pd.to_datetime(df['policy_bind_date'], errors='coerce', dayfirst=True)
df['incident_date'] = pd.to_datetime(df['incident_date'], errors='coerce', dayfirst=True)
df['days_since_incident'] = (pd.to_datetime('today') - df['incident_date']).dt.days
df['policy_duration'] = (pd.to_datetime('today') - df['policy_bind_date']).dt.days
df.drop(columns=['policy_bind_date', 'incident_date'], inplace=True)

# One-hot encode categorical variables
categorical_cols = [
    'policy_state', 'policy_csl', 'insured_sex', 'insured_education_level', 
    'insured_occupation', 'insured_hobbies', 'insured_relationship', 
    'incident_type', 'collision_type', 'incident_severity', 
    'authorities_contacted', 'incident_state', 'incident_city', 
    'incident_location', 'property_damage', 'police_report_available'
]
df = pd.get_dummies(df, columns=categorical_cols, drop_first=False)

# Save master feature list for future datasets
master_feature_list = list(df.drop(columns=['fraud_reported']).columns)
joblib.dump(master_feature_list, "master_feature_list.pkl")

# Scale numeric features
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
joblib.dump(scaler, 'scaler.pkl')

# Split features and target
X = df.drop(columns=['fraud_reported'])
y = df['fraud_reported'].map({'Y': 1, 'N': 0})
print(f"Records after preprocessing: {len(X)}")

# Handle class imbalance
X_train_raw, X_test, y_train_raw, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Training records (before SMOTETomek): {len(X_train_raw)}")
print(f"Testing records: {len(X_test)}")

smote_tomek = SMOTETomek(random_state=42)
X_train, y_train = smote_tomek.fit_resample(X_train_raw, y_train_raw)
print(f"Training records after SMOTETomek: {len(X_train)}")

# Ensure all columns are numeric
X_train = X_train.apply(pd.to_numeric, errors='coerce')
X_test = X_test.apply(pd.to_numeric, errors='coerce')

# Fill any remaining NaN values
X_train.fillna(0, inplace=True)
X_test.fillna(0, inplace=True)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train.astype(np.float32).to_numpy(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test.astype(np.float32).to_numpy(), dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).view(-1, 1)

# Split training data into train and validation sets
X_train_np = X_train_tensor.numpy()
y_train_np = y_train_tensor.numpy()
X_train_sub, X_val, y_train_sub, y_val = train_test_split(
    X_train_np, y_train_np, test_size=0.1, random_state=42, stratify=y_train_np
)
print(f"Training records for model training: {len(X_train_sub)}")
print(f"Validation records: {len(X_val)}")

# Convert back to tensors for DataLoader
X_train_sub_tensor = torch.tensor(X_train_sub, dtype=torch.float32)
y_train_sub_tensor = torch.tensor(y_train_sub, dtype=torch.float32).view(-1, 1)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)

# Create DataLoaders for training and validation
train_dataset = TensorDataset(X_train_sub_tensor, y_train_sub_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

# Define ANN Model
class FraudDetectionANN(nn.Module):
    def __init__(self, input_dim):
        super(FraudDetectionANN, self).__init__()
        self.hidden1 = nn.Linear(input_dim, 128)
        self.batch_norm1 = nn.BatchNorm1d(128)
        self.hidden2 = nn.Linear(128, 64)
        self.batch_norm2 = nn.BatchNorm1d(64)
        self.output = nn.Linear(64, 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = self.hidden1(x)
        x = self.batch_norm1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.hidden2(x)
        x = self.batch_norm2(x)
        x = self.relu(x)
        x = self.output(x)
        return x

# Train Model with Early Stopping
input_dim = X_train_tensor.shape[1]
model = FraudDetectionANN(input_dim)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

epochs = 50
patience = 20
best_val_loss = float('inf')
early_stop_counter = 0
best_model_state = None

for epoch in range(epochs):
    model.train()
    train_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        logits = model(X_batch)
        loss = criterion(logits, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_loader)

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            logits = model(X_batch)
            loss = criterion(logits, y_batch)
            val_loss += loss.item()
    val_loss /= len(val_loader)

    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stop_counter = 0
        best_model_state = model.state_dict()
    else:
        early_stop_counter += 1

    if early_stop_counter >= patience:
        print("Early stopping triggered.")
        break

if best_model_state is not None:
    model.load_state_dict(best_model_state)

# Threshold Tuning and Test Set Evaluation
model.eval()
with torch.no_grad():
    y_test_logits = model(X_test_tensor).squeeze()
    y_test_proba = torch.sigmoid(y_test_logits).numpy()

fpr, tpr, thresholds = roc_curve(y_test_tensor.numpy(), y_test_proba)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
print(f"\nOptimal Threshold: {optimal_threshold:.4f}")

y_test_pred = (y_test_proba >= optimal_threshold).astype(int)
accuracy = accuracy_score(y_test, y_test_pred)
precision = precision_score(y_test, y_test_pred)
recall = recall_score(y_test, y_test_pred)
f1 = f1_score(y_test, y_test_pred)
roc_auc = roc_auc_score(y_test, y_test_proba)

print("\nTest Set Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")

# Calculate and print fraudulent and non-fraudulent predictions
total_test = len(y_test_pred)
fraud_count = np.sum(y_test_pred)
non_fraud_count = total_test - fraud_count

fraud_percentage = (fraud_count / total_test) * 100
non_fraud_percentage = (non_fraud_count / total_test) * 100

print(f"\nNumber of test records predicted as fraudulent: {fraud_count}")
print(f"Number of test records predicted as non-fraudulent: {non_fraud_count}")
print(f"Percentage of test records predicted as fraudulent: {fraud_percentage:.2f}%")
print(f"Percentage of test records predicted as non-fraudulent: {non_fraud_percentage:.2f}%")

torch.save(model.state_dict(), "FraudDetectionModel.pth")
